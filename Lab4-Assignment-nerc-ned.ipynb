{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition, Classification and Disambiguation\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 cis important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* perform feature ablation and gain insight into the contribution of various features\n",
    "* Learn how to evaluate an entity linking system.\n",
    "* Learn how to run two entity linking systems (AIDA and DBpedia Spotlight).\n",
    "* Learn how to interpret the system output and the evaluation results.\n",
    "* Get insight into differences between the two systems.\n",
    "* Be able to describe differences between the two methods in terms of their results.\n",
    "* Be able to propose future improvements based on the observed results.\n",
    "* Get insight into the difficulty of NED and how this depends on specific entity mentions.\n",
    "\n",
    "The assignment consists of 2 parts:\n",
    "\n",
    "* Named Entity Recornition and Classificaiton: excersizes 1 & 2\n",
    "* Named Entity Disambiguation and Linking: excersizes 3 & 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition and Classification\n",
    "\n",
    "Excercises 2 and 3 focus on Named Entity Recognition and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the nerc_datasets folder on your local machine\n",
    "train = ConllCorpusReader(r'C:\\Users\\FCH\\Desktop\\Text_mining\\text-mining-ba\\lab_sessions\\lab4\\data\\CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "       'words': token, 'pos': pos\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the NERC_datasets folder on your local machine\n",
    "test = ConllCorpusReader(r'C:\\Users\\FCH\\Desktop\\Text_mining\\text-mining-ba\\lab_sessions\\lab4\\data\\CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "        'words': token, 'pos': pos\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train set: 203621   Percentage: 0.8143015964423969\n",
      "Items in test set: 46435   Percentage: 0.1856984035576031\n",
      "Ratio Train/Test: 4.385075912565952 \n",
      "\n",
      "B-ORG\n",
      "Training: 6321 \t\t\tPercentage: 0.03104296708099852\n",
      "Test: 1661 \t\t\tPercentage: 0.03577043178636804\n",
      "ratio Training/Test: 3.805538832028898 \n",
      "\n",
      "O\n",
      "Training: 169578 \t\t\tPercentage: 0.832811939829389\n",
      "Test: 38323 \t\t\tPercentage: 0.8253041886508022\n",
      "ratio Training/Test: 4.424966730162044 \n",
      "\n",
      "B-MISC\n",
      "Training: 3438 \t\t\tPercentage: 0.016884309575142052\n",
      "Test: 702 \t\t\tPercentage: 0.015117906751372886\n",
      "ratio Training/Test: 4.897435897435898 \n",
      "\n",
      "B-PER\n",
      "Training: 6600 \t\t\tPercentage: 0.0324131597428556\n",
      "Test: 1617 \t\t\tPercentage: 0.03482287067944438\n",
      "ratio Training/Test: 4.081632653061225 \n",
      "\n",
      "I-PER\n",
      "Training: 4528 \t\t\tPercentage: 0.022237392017522752\n",
      "Test: 1156 \t\t\tPercentage: 0.0248950145364488\n",
      "ratio Training/Test: 3.916955017301038 \n",
      "\n",
      "B-LOC\n",
      "Training: 7140 \t\t\tPercentage: 0.03506514553999833\n",
      "Test: 1668 \t\t\tPercentage: 0.03592118014428771\n",
      "ratio Training/Test: 4.280575539568345 \n",
      "\n",
      "I-ORG\n",
      "Training: 3704 \t\t\tPercentage: 0.018190658134475325\n",
      "Test: 835 \t\t\tPercentage: 0.017982125551846667\n",
      "ratio Training/Test: 4.435928143712575 \n",
      "\n",
      "I-MISC\n",
      "Training: 1155 \t\t\tPercentage: 0.00567230295499973\n",
      "Test: 216 \t\t\tPercentage: 0.004651663615807042\n",
      "ratio Training/Test: 5.347222222222222 \n",
      "\n",
      "I-LOC\n",
      "Training: 1157 \t\t\tPercentage: 0.005682125124618777\n",
      "Test: 257 \t\t\tPercentage: 0.005534618283622268\n",
      "ratio Training/Test: 4.501945525291829 \n",
      "\n",
      "TRAIN DISTRIBUTION\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DISTRIBUTION\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_size = len(training_features)\n",
    "test_size = len(test_features)\n",
    "\n",
    "print('Items in train set:', train_size, '  Percentage:', train_size/(train_size + test_size))\n",
    "print('Items in test set:', test_size, '  Percentage:', test_size/(train_size + test_size))\n",
    "print('Ratio Train/Test:', train_size/test_size, '\\n')\n",
    "\n",
    "count_train = Counter(training_gold_labels)\n",
    "count_test  = Counter(test_gold_labels)\n",
    "\n",
    "for i in count_train.keys():\n",
    "    print(i)\n",
    "    print('Training:', count_train[i], '\\t\\t\\tPercentage:', count_train[i]/train_size)\n",
    "    print('Test:', count_test[i], '\\t\\t\\tPercentage:', count_test[i]/test_size)\n",
    "    print('ratio Training/Test:', count_train[i]/count_test[i], '\\n')\n",
    "\n",
    "\n",
    "def print_counter(counter):\n",
    "    labels, values = zip(*counter.items())\n",
    "\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "\n",
    "    plt.bar(indexes, values, width)\n",
    "    plt.xticks(indexes , labels)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "print('TRAIN DISTRIBUTION')\n",
    "print_counter(count_train)\n",
    "print('TEST DISTRIBUTION')\n",
    "print_counter(count_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a train-test split of 81,4% against 18,6%.\n",
    "- There is a big class imbalance in the data, where the 'O' category is overpopulated. The other classes are equally balanced\n",
    "- The test and train set both have a large overpopulation of the 'O' category while the other categories have similar proportions with some negligible differences. Normally this would be reduced by using a stratified split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "the_array = vec.fit_transform(training_features + test_features)\n",
    "\n",
    "training_ = the_array[:len(training_features)]\n",
    "test_ = the_array[len(training_features):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.78      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.72       702\n",
      "       B-ORG       0.79      0.52      0.63      1661\n",
      "       B-PER       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.57      0.59      0.58       216\n",
      "       I-ORG       0.70      0.47      0.56       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.98      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.72      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_clf = svm.LinearSVC()\n",
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "lin_clf.fit(training_, training_gold_labels)\n",
    "print(classification_report(test_gold_labels, lin_clf.predict(test_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By considering the f1-score, the class that has the best performance is O. We believe this result can be explained by the following reason: The O label has the best performance of all because it is the most abundant and contain every part of text that cannot be related to an entity (such as verbs, adjectives, pronouns, etc.). Those factors make so that for the O class there are more data for learning and that the machine learning algorithm can easily learn to correlate the use of a POS tag not related to an entity to the O label.\n",
    "- The worst label for our classifier is I-per, considering the f1-score. Considering that the algorithm does not have any information about ordering of the tokens in the sequence, it may be confusing the B-per and I-per labels. The beggining and inside of a persons name may have the same vector representation and the only way to desambiguate between the two options is by considering the ordering of the tokens. Take for example the names Jonatan Smith and Smith Jonatan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\FCH\\Desktop\\Text_mining\\text-mining-ba\\lab_sessions\\GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v(word, word_embedding_model):\n",
    "    if word in word_embedding_model:\n",
    "        vector=word_embedding_model[word]\n",
    "    else: \n",
    "        vector=[0]*300\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = []\n",
    "training_gold_labels = []\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    training_features.append(get_w2v(token, word_embedding_model))\n",
    "    training_gold_labels.append(ne_label)\n",
    "\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    test_features.append(get_w2v(token, word_embedding_model))\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.80      0.78      1668\n",
      "      B-MISC       0.72      0.70      0.71       702\n",
      "       B-ORG       0.69      0.64      0.66      1661\n",
      "       B-PER       0.75      0.67      0.71      1617\n",
      "       I-LOC       0.51      0.42      0.46       257\n",
      "      I-MISC       0.60      0.54      0.57       216\n",
      "       I-ORG       0.48      0.33      0.39       835\n",
      "       I-PER       0.59      0.50      0.54      1156\n",
      "           O       0.97      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.62      0.64     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_clf = svm.LinearSVC()\n",
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "lin_clf.fit(training_features, training_gold_labels)\n",
    "print(classification_report(test_gold_labels, lin_clf.predict(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of using embeddings are pretty similar to the ones from the last experiment, the only notable differences are that the f1-score for classification of B-PER is higher for the embedding model, and the f1-scores of I-Loc and I-org are lower for the embedding model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = r'C:\\Users\\FCH\\Desktop\\Text_mining\\text-mining-ba\\lab_sessions\\lab4\\data\\nerc_datasets\\kaggle\\ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050795"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(remove, df_train=df_train, df_test=df_test, training_labels = df_train['tag'], test_labels = df_test['tag']):\n",
    "    training_features = []\n",
    "    test_features = []\n",
    "    to_drop = ['id', 'sentence_idx', 'tag'] + remove\n",
    "    \n",
    "    training_labels = df_train['tag']\n",
    "    test_labels = df_test['tag']   \n",
    "    \n",
    "    df_train = df_train.drop(columns=to_drop)\n",
    "    df_test = df_test.drop(columns=to_drop)\n",
    "\n",
    "    for index, row in df_train.iterrows():\n",
    "        item = row.to_dict()\n",
    "        training_features.append(item)\n",
    "\n",
    "    for index, row in df_test.iterrows():\n",
    "        item = row.to_dict()\n",
    "        test_features.append(item)\n",
    "\n",
    "    vec = DictVectorizer()\n",
    "    the_array = vec.fit_transform(training_features + test_features)\n",
    "\n",
    "    training = the_array[:len(training_features)]\n",
    "    test = the_array[len(training_features):]\n",
    "    \n",
    "    return training, test, training_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FCH\\.conda\\envs\\tm\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         4\n",
      "       B-eve       0.00      0.00      0.00         0\n",
      "       B-geo       0.87      0.84      0.85       741\n",
      "       B-gpe       0.87      0.94      0.90       296\n",
      "       B-nat       0.80      0.50      0.62         8\n",
      "       B-org       0.73      0.66      0.70       397\n",
      "       B-per       0.81      0.81      0.81       333\n",
      "       B-tim       0.93      0.84      0.88       393\n",
      "       I-geo       0.97      0.96      0.97       156\n",
      "       I-gpe       0.67      1.00      0.80         2\n",
      "       I-nat       1.00      1.00      1.00         4\n",
      "       I-org       0.95      0.93      0.94       321\n",
      "       I-per       0.95      0.98      0.96       319\n",
      "       I-tim       0.98      0.87      0.92       108\n",
      "           O       0.99      0.99      0.99     16918\n",
      "\n",
      "    accuracy                           0.97     20000\n",
      "   macro avg       0.77      0.76      0.76     20000\n",
      "weighted avg       0.97      0.97      0.97     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test, train_labels, test_labels = train_test([])\n",
    "lin_clf = svm.LinearSVC()\n",
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "lin_clf.fit(train, train_labels)\n",
    "print(classification_report(test_labels, lin_clf.predict(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the results show a high recall and precision value for all the classification labels. Furthermore, the weighted average is also high (0.97). The main source of errors of the model are the minority classes, which have had fewer training instances and therefore have the lowest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Linking\n",
    "\n",
    "Excersizes 3 and 4 focus on Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersize 3 (NEL): Quantitative analysis  [Points: 15] \n",
    "\n",
    "In this assignment, you are going to work with two systems for entity linking: AIDA and DBpedia Spotlight. You will run them on an entity linking dataset and evaluate their performance. You will perform both quantitative and qualitative analysis of their output, and run one of these systems on your own text. We will reflect on the results of these tasks.\n",
    "\n",
    "**Note:** We will use the dataset Reuters-128 in this assignment. This dataset was introduced in the notebook 'Lab4.3-Entity-linking-tools', so you probably have it already (in case you do not have it make sure you download it from Canvas first and put it in the same location as this notebook). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1a** Write code that runs both systems on the full Reuters-128 dataset. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from tqdm import tqdm ## to create progress bar to measure progress\n",
    "import sys\n",
    "import requests\n",
    "import urllib\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlencode\n",
    "import xml.etree.cElementTree as ET\n",
    "from lxml import etree\n",
    "import time\n",
    "import json\n",
    "# import time\n",
    "# import our own utility functions and classes\n",
    "import lab4_utils as utils\n",
    "import lab4_classes as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "context = ssl._create_unverified_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both systems on the full Reuters-128 dataset\n",
    "def aida_disambiguation(articles, aida_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with AIDA.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:  \n",
    "        for i, article in enumerate(articles):\n",
    "            \n",
    "            original_content = article.content\n",
    "            new_content=original_content\n",
    "            for entity in reversed(article.entity_mentions):\n",
    "                entity_span=new_content[entity.begin_index: entity.end_index]\n",
    "                new_content=new_content[:entity.begin_index] + '[[' + entity_span + ']]' + new_content[entity.end_index:]\n",
    "\n",
    "            params={\"text\": new_content, \"tag_mode\": 'manual'}\n",
    "            request = Request(aida_url, urlencode(params).encode())\n",
    "            this_json = urlopen(request, context = context).read().decode('unicode-escape')\n",
    "            try:\n",
    "                results=json.loads(this_json)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            dis_entities={}\n",
    "            for dis_entity in results['mentions']:\n",
    "\n",
    "                if 'bestEntity' in dis_entity.keys():\n",
    "                    best_entity = dis_entity['bestEntity']['kbIdentifier']\n",
    "                    clean_url = best_entity[5:] #SKIP YAGO:\n",
    "                else:\n",
    "                    clean_url = 'NIL'\n",
    "                dis_entities[str(dis_entity['offset'])] = clean_url \n",
    "            \n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                try:\n",
    "                    dis_url = str(dis_entities[str(start)])  \n",
    "                except:\n",
    "                    dis_url = 'NIL'\n",
    "                entity.aida_link = dis_url\n",
    "\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "    return articles\n",
    "\n",
    "def spotlight_disambiguate(articles, spotlight_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with DBpedia Spotlight.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "            # Similar as with AIDA, we first prepare the document text and the mentions\n",
    "            # in order to provide these to Spotlight as input.\n",
    "            \n",
    "            # We build up the XML structure that Spotligh wants as input\n",
    "            # The next function Element creates the XML element with the text attribute\n",
    "            annotation = etree.Element(\"annotation\", text=article.content)\n",
    "            \n",
    "            # We iterate over the eneity mentions from our Reuters data to create the surface form elements\n",
    "            for mention in article.entity_mentions:\n",
    "                sf = etree.SubElement(annotation, \"surfaceForm\")\n",
    "                sf.set(\"name\", mention.mention)\n",
    "                sf.set(\"offset\", str(mention.begin_index))\n",
    "            my_xml=etree.tostring(annotation, xml_declaration=True, encoding='UTF-8')\n",
    "            # Send a disambiguation request to spotlight\n",
    "            results=requests.post(spotlight_url, urllib.parse.urlencode({'text':my_xml, 'confidence': 0.5}), \n",
    "                                  headers={'Accept': 'application/json'})\n",
    "            # Note that you can adjust the confidence value. Check the online demo to see the effect. \n",
    "            # What will happen with the recall and precision if you increase the confidence?\n",
    "            \n",
    "            # Process the results and normalize the entity URIs\n",
    "            \n",
    "            if not results:\n",
    "                continue\n",
    "            \n",
    "            j=results.json()\n",
    "            dis_entities={}\n",
    "            if 'Resources' in j: \n",
    "                resources=j['Resources']\n",
    "            else: \n",
    "                resources=[]\n",
    "            for dis_entity in resources:\n",
    "                dis_entities[str(dis_entity['@offset'])] = utils.normalizeURL(dis_entity['@URI'])\n",
    "            \n",
    "            # Let's now store the URLs by Spotlight to our class for later analysis.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                if str(start) in dis_entities:\n",
    "                    dis_url = dis_entities[str(start)]\n",
    "                else:\n",
    "                    dis_url = 'NIL'\n",
    "                entity.spotlight_link = dis_url\n",
    "    \n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "                \n",
    "            # Pause for 100ms to prevent overloading the server\n",
    "            time.sleep(0.1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1:   1%|▌                                                                   | 1/128 [00:04<08:36,  4.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FCH\\.conda\\envs\\tm\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 128:  96%|█████████████████████████████████████████████████████████████▌  | 123/128 [03:26<00:08,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "reuters_file = 'Reuters-128.ttl'\n",
    "aida_disambiguation_url = \"https://gate.d5.mpi-inf.mpg.de/aida/service/disambiguate\"\n",
    "spotlight_disambiguation_url = \"http://model.dbpedia-spotlight.org/en/disambiguate\"\n",
    "\n",
    "articles = utils.load_article_from_nif_file(reuters_file)\n",
    "processed_aida = aida_disambiguation(articles, aida_disambiguation_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 64:  47%|██████████████████████████████▉                                   | 60/128 [01:22<01:33,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "processed_spotlight =spotlight_disambiguate(processed_aida, spotlight_disambiguation_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b** Write code that evaluates the two systems on this dataset by computing their overall precision, recall, and F1-score. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the precision, recall, and F1-score for each of the systems on this dataset\n",
    "\n",
    "def evaluate_entity_linking(system_decisions, gold_decisions):\n",
    "    \n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    \n",
    "    for gold_entity,system_entity in zip(gold_decisions,system_decisions):\n",
    "        if gold_entity=='NIL' and system_entity=='NIL': continue\n",
    "        if gold_entity==system_entity:\n",
    "            tp+=1\n",
    "        else:\n",
    "            if gold_entity!='NIL':\n",
    "                fn+=1\n",
    "            if system_entity!='NIL':\n",
    "                fp+=1\n",
    "\n",
    "#     print('TP: %d; \\nFP: %d, \\nFN: %d' % (tp, fp, fn))            \n",
    "\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIDA System:\n",
      "Precision: 0.61875  Recall: 0.45692307692307693  F1: 0.5256637168141594\n",
      "Spotlight System:\n",
      "Precision: 0.61875  Recall: 0.45692307692307693  F1: 0.5256637168141594\n"
     ]
    }
   ],
   "source": [
    "decisions_aida = []\n",
    "gold_aida = []\n",
    "for article in processed_aida:\n",
    "    decisions_aida += [entity.aida_link for entity in article.entity_mentions]\n",
    "    gold_aida += [entity.gold_link for entity in article.entity_mentions]\n",
    "\n",
    "decisions_spotlight = []\n",
    "gold_spotlight = []\n",
    "for article in processed_spotlight:\n",
    "    decisions_spotlight += [entity.spotlight_link for entity in article.entity_mentions]\n",
    "    gold_spotlight += [entity.gold_link for entity in article.entity_mentions]\n",
    "    \n",
    "precision_a, recall_a, f1_a = evaluate_entity_linking(decisions_aida, gold_aida)\n",
    "print('AIDA System:\\nPrecision:', precision_a, ' Recall:', recall_a, ' F1:', f1_a)\n",
    "precision_s, recall_s, f1_s = evaluate_entity_linking(decisions_aida, gold_spotlight)\n",
    "print('Spotlight System:\\nPrecision:', precision_s, ' Recall:', recall_s, ' F1:', f1_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lab4_classes.EntityMention at 0x26806caadc8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_spotlight[0].entity_mentions[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1c** What is the F1-score per system? Which system performs better? Is that also the better system in terms of precision and recall? Which is higher and what does that mean (hint: think of NIL entities)?(5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our results, both systems have the same performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersize 4 (NEL): Qualitative analysis [Points: 15] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2a** Check the entity disambiguation by AIDA against the gold entities on the document with identifier \"http://aksw.org/N3/Reuters-128/82#char=0,1370\" (write code to print the entity mentions, gold links and AIDA links). (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_article = [article for article in articles if article.identifier == 'http://aksw.org/N3/Reuters-128/82#char=0,1370'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTITY GOLD : Reuters , ENTITY AIDA : Reuters_Group\n",
      "ENTITY GOLD : NIL , ENTITY AIDA : NIL\n",
      "ENTITY GOLD : Tokyo_Stock_Exchange , ENTITY AIDA : Tokyo\n",
      "ENTITY GOLD : London_Stock_Exchange , ENTITY AIDA : NIL\n",
      "ENTITY GOLD : NIL , ENTITY AIDA : Edward_Hogg\n",
      "ENTITY GOLD : Reuters , ENTITY AIDA : Reuters\n",
      "ENTITY GOLD : London , ENTITY AIDA : London\n"
     ]
    }
   ],
   "source": [
    "for e in correct_article.entity_mentions:\n",
    "    print(\"ENTITY GOLD :\", e.gold_link,\", ENTITY AIDA :\", e.aida_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdflib.term.Literal('Exchanges and telecommunications authorities should abolish their restrictions on full and free dissemination of information to the investment and banking communities, Reuters Holdings Plc RTRS.L chairman Sir Christopher Hogg said. In the 1986 annual repoprt, he said lengthy negotiations had brought agreement with the Tokyo and London Stock Exchanges for fuller, but still not complete, access to market data through Reuter services. Many other markets maintain restrictions, he added. Hogg said members of some markets appear to believe that information restrictions protected their interests. In other cases, exchanges seem to be limiting the distribution of data in order to provide competitive advantage to their own commercial information businesses. He also noted that despite increasing liberalisation in the telecommunications field, some countries continue to protect their state monopolies at the expense of other economic sectors. Reuter dealing services remain excluded from such countries. As a result, banking communities serving entire economies are put at a competitive disadvantage, he added. Reuters increased its 1986 pre-tax profit by 39 pct from the previous year to 130.1 mln stg on a 43 pct rise in revenues to 620.9 mln stg. Earnings per ordinary share were up 47 pct to 19.4p. The annual shareholder meeting will be held in London on April 29.', lang='en')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_article.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in this document that one of the mentions of \"Tokyo\" is disambiguated wrongly by AIDA as `Tokyo` (it should be `Tokyo_Stock_Exchange`). Knowing how AIDA works, what would be your explanation for this error? (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible explanation for this error is that the entities that are mentioned in the surrounding context of the word Tokyo, such as London and Reuters, are more often connected with the entity Tokyo (which means those entities have connections with higher weights). And since the AIDA algorithm ranks entities based on this connectivity weight Tokyo got predicted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2b** Check the entity disambiguation by Spotlight against the gold entities on the document \"http://aksw.org/N3/Reuters-128/36#char=0,1146\" (write code to print the entity mentions, gold links and Spotlight links). (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_article = [article for article in articles if article.identifier == 'http://aksw.org/N3/Reuters-128/36#char=0,1146'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: U.S. Treasury , SPOTLIGHT:  United_States_Department_of_the_Treasury , GL: United_States_Department_of_the_Treasury\n",
      "entity: Group of Five , SPOTLIGHT:  Group_of_Five , GL: Group_of_Five\n",
      "entity: Gerhard Stoltenberg , SPOTLIGHT:  Gerhard_Stoltenberg , GL: Gerhard_Stoltenberg\n",
      "entity: Bundesbank , SPOTLIGHT:  German_Federal_Bank , GL: Deutsche_Bundesbank\n",
      "entity: Karl Otto Poehl , SPOTLIGHT:  NIL , GL: Karl_Otto_Pöhl\n",
      "entity: Edouard Balladur , SPOTLIGHT:  Édouard_Balladur , GL: Édouard_Balladur\n",
      "entity: Jacques de Larosiere , SPOTLIGHT:  NIL , GL: Jacques_de_Larosière\n",
      "entity: Kiichi Miyazawa , SPOTLIGHT:  Kiichi_Miyazawa , GL: Kiichi_Miyazawa\n",
      "entity: Satoshi Sumita , SPOTLIGHT:  NIL , GL: Satoshi_Sumita\n",
      "entity: Robin Leigh Pemberton , SPOTLIGHT:  NIL , GL: Robin_Leigh-Pemberton,_Baron_Kingsdown\n",
      "entity: Group of Seven , SPOTLIGHT:  Group_of_Seven , GL: G7\n",
      "entity: Giovanni Goria , SPOTLIGHT:  Giovanni_Goria , GL: Giovanni_Goria\n",
      "entity: Treasury , SPOTLIGHT:  HM_Treasury , GL: United_States_Department_of_the_Treasury\n",
      "entity: James Baker , SPOTLIGHT:  James_Baker , GL: James_Baker\n",
      "entity: Baker , SPOTLIGHT:  James_Baker , GL: James_Baker\n",
      "entity: Goria , SPOTLIGHT:  Giovanni_Goria , GL: Giovanni_Goria\n",
      "entity: Group of Seven , SPOTLIGHT:  Group_of_Seven , GL: G7\n",
      "entity: Paris , SPOTLIGHT:  Paris , GL: Paris\n",
      "entity: Italy , SPOTLIGHT:  Kingdom_of_Italy , GL: Italy\n"
     ]
    }
   ],
   "source": [
    "for entity in correct_article.entity_mentions:\n",
    "    print('entity:', entity.mention ,', SPOTLIGHT: ', entity.spotlight_link, ', GL:', entity.gold_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in this document that the mention of \"Group of Seven\" is disambiguated wrongly by Spotlight as `G8` (it should be `G7`). Knowing how Spotlight works, what would be your explanation for this error? (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our results such mistake is not found. Nonetheless, one possible explanation for the error is that during the disambiguation phase, the information retrieval vector representation of G8 was closer to query vector. Indeed, the G8 and G7 groups are often discribed with similar words, since both are governamental groups with almost all the same members and political agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2c** In the document with identifier \"http://aksw.org/N3/Reuters-128/67#char=0,1627\":\n",
    "- both systems correctly decide that \"Michel Dufour\" is a `NIL` entity with no representation in the English Wikipedia. \n",
    "- however, Spotlight later decides that \"Dufour\" refers to `Guillaume-Henri_Dufour`\n",
    "\n",
    "How would you help Spotlight fix this error? (Hint: think of how you would know that \"Dufour\" is a NIL entity in that document) (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1: 100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "processed: 1: 100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "entity: Dominion Textile Inc , SPOTLIGHT: NIL , AIDA: NIL , GL: Dominion_Textile\n",
      "entity: Burlington Industries Inc , SPOTLIGHT: NIL , AIDA: NIL , GL: Burlington_Industries\n",
      "entity: Michel Dufour , SPOTLIGHT: NIL , AIDA: NIL , GL: NIL\n",
      "entity: Reuters , SPOTLIGHT: Reuters , AIDA: Reuters , GL: Reuters\n",
      "entity: Dominion Textile , SPOTLIGHT: Dominion_Textile , AIDA: Dominion_Textile , GL: Dominion_Textile\n",
      "entity: Dufour , SPOTLIGHT: Guillaume-Henri_Dufour , AIDA: Antoine_Dufour , GL: NIL\n",
      "entity: Dominion Textile , SPOTLIGHT: Dominion_Textile , AIDA: Dominion_Textile , GL: Dominion_Textile\n",
      "entity: Thomas Bell , SPOTLIGHT: Thom_Bell , AIDA: Thom_Bell , GL: NIL\n",
      "entity: Dominion Textile , SPOTLIGHT: Dominion_Textile , AIDA: Dominion_Textile , GL: Dominion_Textile\n",
      "entity: Avondale Mills , SPOTLIGHT: Avondale_Mills , AIDA: NIL , GL: NIL\n",
      "entity: Dufour , SPOTLIGHT: Guillaume-Henri_Dufour , AIDA: Antoine_Dufour , GL: NIL\n",
      "entity: Burlington Industries , SPOTLIGHT: Burlington_Industries , AIDA: Burlington_Industries , GL: Burlington_Industries\n",
      "entity: Dominion Textile , SPOTLIGHT: Dominion_Textile , AIDA: Dominion_Textile , GL: Dominion_Textile\n",
      "entity: Asher Edelman , SPOTLIGHT: Asher_Edelman , AIDA: Asher_Edelman , GL: Asher_Edelman\n",
      "entity: Dominion Textile , SPOTLIGHT: Dominion_Textile , AIDA: Dominion_Textile , GL: Dominion_Textile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FCH\\.conda\\envs\\tm\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    }
   ],
   "source": [
    "correct_article = [article for article in articles if article.identifier == 'http://aksw.org/N3/Reuters-128/67#char=0,1627'][0]\n",
    "processed_both = spotlight_disambiguate([correct_article], spotlight_disambiguation_url)\n",
    "processed_aida=aida_disambiguation([correct_article], aida_disambiguation_url)\n",
    "\n",
    "for entity in processed_both[0].entity_mentions:\n",
    "    print('entity:', entity.mention ,', SPOTLIGHT:', entity.spotlight_link, ', AIDA:', entity.aida_link, ', GL:', entity.gold_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying coreference resolution to the text, it would be possible to associate both mentions of \"Dufour\" to the same entity (NIL). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
