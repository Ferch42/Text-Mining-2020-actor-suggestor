{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answers:**\n",
    "* sentence 1: No negative words, so neg is 0.0. Other words are neutral or positive so it results in a postive compound score based on how postive the words are.\n",
    "* sentence 2: Large portion of the sentence is seen as negative because of 'don't', it is strange that because don't was added now 63% of the sentence is negative. The overall score could have been higher since don't love is not very negative.\n",
    "* sentence 3: Score is higher than sentence 1, because ':-)' with a lexicon of 1.3 adds a lot of positivity to the sentence.\n",
    "* sentence 4: 'ruins' doesn't necessarily mean that is is negative. It can be monumental as well. It should be more neutral than negative. Should be based on context and not just the word itself. ('ruins' has a score of -1.9)\n",
    "* sentence 5: This is a clear example of the negation used by VADER. 'not ruins' is seen as positive.\n",
    "* sentence 6: 'lies' is seen as negative, probably because people annotaded the word as negative because of the association with someone lying and not someone lying down. VADER doesn't really take into account what the word means in different contexts.\n",
    "* sentence 7: Probably more on the positive side because of the word like, but would have been more logical if it was 100% neutral, since it is a statement and doesn't say anything about the house.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. \n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T13:50:30.818381Z",
     "start_time": "2020-02-27T13:50:30.815477Z"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T13:50:31.059015Z",
     "start_time": "2020-02-27T13:50:31.055258Z"
    }
   },
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T13:50:31.293592Z",
     "start_time": "2020-02-27T13:50:31.289487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'negative', 'text_of_tweet': 'They spied on my campaign!', 'tweet_url': 'https://twitter.com/realDonaldTrump/status/1232908958421131264'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T13:49:55.813467Z",
     "start_time": "2020-02-27T13:49:51.512424Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load('en_core_web_sm') # en_core_web_sm\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets_train = load_files(str(airline_tweets_folder))\n",
    "\n",
    "data = airline_tweets_train.data\n",
    "# 0:negative, 1:neutral, 2:positive\n",
    "labels = airline_tweets_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T13:49:55.824952Z",
     "start_time": "2020-02-27T13:49:55.815829Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=set(),\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -empty set -> all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T13:50:32.218129Z",
     "start_time": "2020-02-27T13:50:32.210315Z"
    }
   },
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "def vader_output_to_label_num(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 0\n",
    "    elif compound == 0.0:\n",
    "        return 1\n",
    "    elif compound > 0.0:\n",
    "        return 2\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T13:50:33.407435Z",
     "start_time": "2020-02-27T13:50:32.837464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.76      0.84        21\n",
      "     neutral       0.60      0.55      0.57        11\n",
      "    positive       0.65      0.83      0.73        18\n",
      "\n",
      "    accuracy                           0.74        50\n",
      "   macro avg       0.73      0.71      0.72        50\n",
      "weighted avg       0.76      0.74      0.74        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet)\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: negative , Predicted label: neutral\n",
      "They spied on my campaign!\n",
      "--------------------------------------------------------\n",
      "True label: negative , Predicted label: positive\n",
      "There has rarely been a juror so tainted as the forewoman in the Roger Stone case. Look at her background. She never revealed her hatred of “Trump” and Stone. She was totally biased, as is the judge. Roger wasn’t even working on my campaign. Miscarriage of justice. Sad to watch!\n",
      "--------------------------------------------------------\n",
      "True label: negative , Predicted label: neutral\n",
      "Car drives into crowd during carnival procession in German town of Volkmarsen, injuring several people, police say\n",
      "--------------------------------------------------------\n",
      "True label: negative , Predicted label: positive\n",
      "Supporter of Islamic State group admits plotting to bomb London's St Paul's Cathedral and a hotel\n",
      "--------------------------------------------------------\n",
      "True label: neutral , Predicted label: positive\n",
      "SparkLabs Group launches Connex, an accelerator program for smart city technology https://tcrn.ch/2TkGdwT by @catherineshu\n",
      "--------------------------------------------------------\n",
      "True label: positive , Predicted label: neutral\n",
      "Storytelling community Wattpad embraces adult content with new personalization tools\n",
      "--------------------------------------------------------\n",
      "True label: positive , Predicted label: negative\n",
      "Majestic mountain views paired with unique flavors make these wineries must-see stops on your itinerary.\n",
      "--------------------------------------------------------\n",
      "True label: neutral , Predicted label: positive\n",
      "The @bcs has a mature history of embedding ethical approaches into computer science. Read how a new joint-study with @CDEIUK looking to develop practical guidance on embedding ethical practice in MSc courses on #AI, #MachineLearning & #DataScience.\n",
      "--------------------------------------------------------\n",
      "True label: neutral , Predicted label: positive\n",
      "Today we've launched the draft of our #AI auditing guidance. This pivotal piece of work will help in assessing the risks to rights and freedoms that AI can cause; and the appropriate measures you can implement to mitigate them.\n",
      "--------------------------------------------------------\n",
      "True label: neutral , Predicted label: positive\n",
      "#TBT This week we look back to our 2004 interview with Christian Pfeiffer, the Four-time world stunt riding champion!\n",
      "--------------------------------------------------------\n",
      "True label: neutral , Predicted label: positive\n",
      "Dresses, we can't have enough of them.. Grijnzend gezicht Prepare your summer with some heels for a fancy dinner or with casual sneakers on a shopping trip! \n",
      "--------------------------------------------------------\n",
      "True label: positive , Predicted label: neutral\n",
      "Hello new Spring/Summer kids collection! Get your 20% discount on all items, only two days left…\n",
      "--------------------------------------------------------\n",
      "True label: negative , Predicted label: positive\n",
      "Clive Cussler, the million-selling adventure writer and real-life thrill seeker who wove personal details and spectacular fantasies into his page-turning novels about underwater explorer Dirk Pitt, has died.\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tweets)):\n",
    "    if all_vader_output[i]!= gold[i]:\n",
    "        print('True label:', gold[i], ', Predicted label:', all_vader_output[i])\n",
    "        print(tweets[i])\n",
    "        print('--------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answers:**\n",
    "- Overall the classification reports shows that the class negative has the best f1-score and precision, while the positive class has the highest recall. This means that negative tweets are more correclty classified by VADER, and that the positive tweets are detected more frequently. Nonetheless, VADER performs poorly when considering the neutral class, compared to the other two classes.\n",
    "- Most of the errors encountered above are of VADER switching neutral tweets as positive ones. This may explained by the fact that VADER scores words independently from their context, which makes so that if a neutral tweet has words of positive conotation, the compound score may be inflated by those positive words. The confusion between positive and negative tweets may be due to the fact that those texts employ words out of the VADER lexicon, such as :\"spied, misscariage\", which have a negative sentiment, but were not considered by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T14:30:26.657539Z",
     "start_time": "2020-02-18T14:29:39.597364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.51      0.63      1750\n",
      "           1       0.60      0.51      0.55      1515\n",
      "           2       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER (as it is) on the set of airline tweets\n",
    "eval_v = [vader_output_to_label_num(run_vader(str(tweet))) for tweet in data]\n",
    "print(classification_report(labels, eval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T14:31:10.671815Z",
     "start_time": "2020-02-18T14:30:26.659546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.52      0.63      1750\n",
      "           1       0.60      0.50      0.54      1515\n",
      "           2       0.56      0.87      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VADER on the set of airline tweets after having lemmatized the text\n",
    "eval_v = [vader_output_to_label_num(run_vader(str(tweet), lemmatize=True)) for tweet in data]\n",
    "print(classification_report(labels, eval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T14:34:08.752306Z",
     "start_time": "2020-02-18T14:33:23.706732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.21      0.34      1750\n",
      "           1       0.41      0.89      0.56      1515\n",
      "           2       0.67      0.45      0.54      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.52      0.48      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run VADER on the set of airline tweets with only adjectives\n",
    "eval_v = [vader_output_to_label_num(run_vader(str(tweet), parts_of_speech_to_consider={'ADJ'})) for tweet in data]\n",
    "print(classification_report(labels, eval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T14:35:31.059036Z",
     "start_time": "2020-02-18T14:34:39.506283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.21      0.34      1750\n",
      "           1       0.41      0.89      0.56      1515\n",
      "           2       0.67      0.45      0.54      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.52      0.48      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "eval_v = [vader_output_to_label_num(run_vader(str(tweet), \n",
    "                                              parts_of_speech_to_consider={'ADJ'},\n",
    "                                              lemmatize=True)) for tweet in data]\n",
    "print(classification_report(labels, eval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T14:36:23.183685Z",
     "start_time": "2020-02-18T14:35:31.061619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.13      0.23      1750\n",
      "           1       0.36      0.82      0.50      1515\n",
      "           2       0.54      0.33      0.41      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.54      0.43      0.38      4755\n",
      "weighted avg       0.55      0.42      0.37      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run VADER on the set of airline tweets with only nouns\n",
    "eval_v = [vader_output_to_label_num(run_vader(str(tweet), parts_of_speech_to_consider={'NOUN'})) for tweet in data]\n",
    "print(classification_report(labels, eval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T14:37:08.252924Z",
     "start_time": "2020-02-18T14:36:23.186215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.15      0.24      1750\n",
      "           1       0.36      0.81      0.50      1515\n",
      "           2       0.53      0.33      0.40      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.53      0.43      0.38      4755\n",
      "weighted avg       0.54      0.42      0.37      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "eval_v = [vader_output_to_label_num(run_vader(str(tweet), \n",
    "                                              parts_of_speech_to_consider={'NOUN'},\n",
    "                                              lemmatize=True)) for tweet in data]\n",
    "print(classification_report(labels, eval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T14:37:51.003337Z",
     "start_time": "2020-02-18T14:37:08.254846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.29      0.42      1750\n",
      "           1       0.38      0.81      0.52      1515\n",
      "           2       0.57      0.34      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.58      0.48      0.46      4755\n",
      "weighted avg       0.59      0.47      0.45      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run VADER on the set of airline tweets with only verbs\n",
    "eval_v = [vader_output_to_label_num(run_vader(str(tweet), parts_of_speech_to_consider={'VERB'})) for tweet in data]\n",
    "print(classification_report(labels, eval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T14:38:39.937950Z",
     "start_time": "2020-02-18T14:37:51.005374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.29      0.42      1750\n",
      "           1       0.38      0.78      0.51      1515\n",
      "           2       0.57      0.35      0.44      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.56      0.48      0.45      4755\n",
      "weighted avg       0.57      0.47      0.45      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "eval_v = [vader_output_to_label_num(run_vader(str(tweet), \n",
    "                                              parts_of_speech_to_consider={'VERB'},\n",
    "                                              lemmatize=True)) for tweet in data]\n",
    "print(classification_report(labels, eval_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***answer:***\n",
    "- It seems that lemmatization has a small effect on model performance, but it is negligible. This result may be explained by the fact that airplane tweets don't have much word variations that can be reduced to their lemmas. \n",
    "- The overall result of adding the POS tags makes that the weighted average for the classification is lower for all cases. Nonetheless, the addition of the POS tags makes so that the precision of the classification of positive and negative tweets increases when compared to the baseline. The precision of neutral cases is lower for the POS models however. This result makes sense because the addition of POS information, specially ADJ, VERB, NOUN can potentially convey sentiment, as adjectives, verbs and nouns often express a lot of semantical emotion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T19:40:22.533750Z",
     "start_time": "2020-02-18T19:40:16.057910Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FCH\\.conda\\envs\\tm\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\FCH\\.conda\\envs\\tm\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\FCH\\.conda\\envs\\tm\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def create_count_vec(tweets, min_df=2):\n",
    "    vec = CountVectorizer(min_df=min_df,\n",
    "                                 tokenizer=nltk.word_tokenize, \n",
    "                                 stop_words=stopwords.words('english'))\n",
    "    count_vec = vec.fit_transform(tweets.data)\n",
    "    return count_vec\n",
    "\n",
    "def create_tfidf_vec(tweets, min_df=2):\n",
    "    count_vec = create_count_vec(tweets, min_df)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf_vec = tfidf_transformer.fit_transform(count_vec)\n",
    "    return tfidf_vec\n",
    "\n",
    "#create vectors\n",
    "vec_2 = create_count_vec(airline_tweets_train)\n",
    "vec_5 = create_count_vec(airline_tweets_train, min_df=5)\n",
    "vec_10 = create_count_vec(airline_tweets_train, min_df=10)\n",
    "\n",
    "tfidf_vec_2 = create_tfidf_vec(airline_tweets_train)\n",
    "tfidf_vec_5 = create_tfidf_vec(airline_tweets_train, min_df=5)\n",
    "tfidf_vec_10 = create_tfidf_vec(airline_tweets_train, min_df=10)\n",
    "\n",
    "def train_and_test_nb(vec, tweets):\n",
    "    #split data\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        vec, \n",
    "        tweets.target, \n",
    "        test_size = 0.2\n",
    "        ) \n",
    "    #train\n",
    "    clf = MultinomialNB().fit(docs_train, y_train)\n",
    "    y_pred = clf.predict(docs_test)\n",
    "    \n",
    "    return classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T19:40:26.565476Z",
     "start_time": "2020-02-18T19:40:26.502977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words, min_df=2:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       344\n",
      "           1       0.87      0.73      0.80       305\n",
      "           2       0.82      0.84      0.83       302\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.83      0.83       951\n",
      " \n",
      "\n",
      "Bag of Words, min_df=5:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88       349\n",
      "           1       0.82      0.77      0.80       304\n",
      "           2       0.83      0.83      0.83       298\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.83      0.84       951\n",
      "weighted avg       0.84      0.84      0.84       951\n",
      " \n",
      "\n",
      "Bag of Words, min_df=10:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89       335\n",
      "           1       0.83      0.81      0.82       312\n",
      "           2       0.87      0.82      0.84       304\n",
      "\n",
      "    accuracy                           0.85       951\n",
      "   macro avg       0.85      0.85      0.85       951\n",
      "weighted avg       0.85      0.85      0.85       951\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "TF-IDF, min_df=2:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87       353\n",
      "           1       0.86      0.69      0.76       315\n",
      "           2       0.78      0.85      0.81       283\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.82      0.81       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      " \n",
      "\n",
      "TF-IDF, min_df=5:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87       350\n",
      "           1       0.80      0.76      0.78       290\n",
      "           2       0.87      0.82      0.84       311\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      " \n",
      "\n",
      "TF-IDF, min_df=10:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86       351\n",
      "           1       0.84      0.70      0.77       305\n",
      "           2       0.86      0.83      0.85       295\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.82      0.82       951\n",
      "weighted avg       0.83      0.83      0.82       951\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Bag of Words, min_df=2:\\n', train_and_test_nb(vec_2, airline_tweets_train), '\\n')\n",
    "print('Bag of Words, min_df=5:\\n', train_and_test_nb(vec_5, airline_tweets_train), '\\n')\n",
    "print('Bag of Words, min_df=10:\\n', train_and_test_nb(vec_10, airline_tweets_train), '\\n')\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "print('TF-IDF, min_df=2:\\n', train_and_test_nb(tfidf_vec_2, airline_tweets_train), '\\n')\n",
    "print('TF-IDF, min_df=5:\\n', train_and_test_nb(tfidf_vec_5, airline_tweets_train), '\\n')\n",
    "print('TF-IDF, min_df=10:\\n', train_and_test_nb(tfidf_vec_10, airline_tweets_train), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***answer***:\n",
    "- Overall the classification of negative tweets has the highest f1-score, independently of the setting.\n",
    "- The document frequency of words in general does not have any effect on the results. This may be explained by the fact that the dataset is so big that most of the words can pass this threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T15:54:53.675949Z",
     "start_time": "2020-02-18T15:54:52.482371Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FCH\\.conda\\envs\\tm\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 1504.0 @\n",
      "0 1376.0 united\n",
      "0 1249.0 .\n",
      "0 431.0 ``\n",
      "0 392.0 flight\n",
      "0 370.0 ?\n",
      "0 357.0 !\n",
      "0 317.0 #\n",
      "0 215.0 n't\n",
      "0 160.0 ''\n",
      "0 128.0 's\n",
      "0 114.0 service\n",
      "0 101.0 virginamerica\n",
      "0 101.0 :\n",
      "0 97.0 cancelled\n",
      "0 96.0 get\n",
      "0 91.0 customer\n",
      "0 90.0 ...\n",
      "0 88.0 bag\n",
      "0 82.0 plane\n",
      "0 78.0 time\n",
      "0 77.0 delayed\n",
      "0 75.0 -\n",
      "0 73.0 ;\n",
      "0 73.0 'm\n",
      "0 67.0 hours\n",
      "0 65.0 late\n",
      "0 65.0 &\n",
      "0 64.0 gate\n",
      "0 63.0 http\n",
      "0 61.0 still\n",
      "0 60.0 amp\n",
      "0 59.0 2\n",
      "0 58.0 hour\n",
      "0 58.0 airline\n",
      "0 57.0 would\n",
      "0 56.0 help\n",
      "0 55.0 ca\n",
      "0 54.0 worst\n",
      "0 50.0 one\n",
      "0 49.0 flights\n",
      "0 49.0 $\n",
      "0 47.0 like\n",
      "0 46.0 waiting\n",
      "0 44.0 delay\n",
      "0 43.0 've\n",
      "0 42.0 never\n",
      "0 42.0 flightled\n",
      "0 42.0 3\n",
      "0 42.0 (\n",
      "0 40.0 people\n",
      "0 40.0 back\n",
      "0 39.0 luggage\n",
      "0 39.0 ever\n",
      "0 39.0 )\n",
      "0 38.0 us\n",
      "0 38.0 check\n",
      "0 37.0 lost\n",
      "0 37.0 due\n",
      "0 36.0 trying\n",
      "0 35.0 u\n",
      "0 35.0 really\n",
      "0 35.0 fly\n",
      "0 35.0 day\n",
      "0 35.0 bags\n",
      "0 33.0 wait\n",
      "0 33.0 seat\n",
      "0 32.0 guys\n",
      "0 31.0 thanks\n",
      "0 31.0 seats\n",
      "0 31.0 days\n",
      "0 31.0 crew\n",
      "0 30.0 ticket\n",
      "0 30.0 need\n",
      "0 30.0 going\n",
      "0 30.0 4\n",
      "0 29.0 got\n",
      "0 29.0 even\n",
      "0 29.0 airport\n",
      "0 28.0 hold\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 1407.0 @\n",
      "1 534.0 .\n",
      "1 507.0 ?\n",
      "1 302.0 jetblue\n",
      "1 296.0 :\n",
      "1 263.0 southwestair\n",
      "1 260.0 united\n",
      "1 246.0 ``\n",
      "1 234.0 flight\n",
      "1 234.0 #\n",
      "1 200.0 americanair\n",
      "1 187.0 http\n",
      "1 179.0 !\n",
      "1 152.0 usairways\n",
      "1 140.0 's\n",
      "1 82.0 get\n",
      "1 78.0 virginamerica\n",
      "1 78.0 -\n",
      "1 63.0 please\n",
      "1 63.0 flights\n",
      "1 61.0 ''\n",
      "1 58.0 help\n",
      "1 58.0 )\n",
      "1 57.0 n't\n",
      "1 55.0 ...\n",
      "1 52.0 need\n",
      "1 52.0 (\n",
      "1 48.0 ;\n",
      "1 46.0 us\n",
      "1 44.0 &\n",
      "1 43.0 tomorrow\n",
      "1 43.0 dm\n",
      "1 41.0 ”\n",
      "1 41.0 fleet\n",
      "1 41.0 fleek\n",
      "1 40.0 “\n",
      "1 40.0 would\n",
      "1 39.0 know\n",
      "1 39.0 flying\n",
      "1 35.0 thanks\n",
      "1 35.0 'm\n",
      "1 34.0 amp\n",
      "1 33.0 hi\n",
      "1 31.0 cancelled\n",
      "1 30.0 way\n",
      "1 29.0 could\n",
      "1 29.0 airport\n",
      "1 28.0 change\n",
      "1 26.0 today\n",
      "1 26.0 one\n",
      "1 26.0 fly\n",
      "1 25.0 number\n",
      "1 24.0 time\n",
      "1 24.0 see\n",
      "1 24.0 like\n",
      "1 24.0 check\n",
      "1 23.0 rt\n",
      "1 23.0 new\n",
      "1 22.0 travel\n",
      "1 22.0 next\n",
      "1 22.0 make\n",
      "1 22.0 go\n",
      "1 22.0 destinationdragons\n",
      "1 21.0 sent\n",
      "1 21.0 guys\n",
      "1 20.0 ticket\n",
      "1 20.0 follow\n",
      "1 20.0 chance\n",
      "1 20.0 back\n",
      "1 20.0 2\n",
      "1 19.0 trying\n",
      "1 19.0 question\n",
      "1 19.0 going\n",
      "1 19.0 first\n",
      "1 18.0 use\n",
      "1 18.0 lax\n",
      "1 18.0 add\n",
      "1 17.0 want\n",
      "1 17.0 seat\n",
      "1 17.0 reservation\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 1330.0 @\n",
      "2 1056.0 !\n",
      "2 762.0 .\n",
      "2 309.0 southwestair\n",
      "2 290.0 #\n",
      "2 284.0 jetblue\n",
      "2 267.0 thanks\n",
      "2 259.0 thank\n",
      "2 257.0 united\n",
      "2 241.0 ``\n",
      "2 182.0 flight\n",
      "2 173.0 :\n",
      "2 171.0 americanair\n",
      "2 136.0 usairways\n",
      "2 131.0 great\n",
      "2 94.0 )\n",
      "2 89.0 service\n",
      "2 80.0 virginamerica\n",
      "2 76.0 love\n",
      "2 72.0 http\n",
      "2 68.0 guys\n",
      "2 67.0 best\n",
      "2 66.0 customer\n",
      "2 63.0 much\n",
      "2 59.0 's\n",
      "2 58.0 awesome\n",
      "2 58.0 -\n",
      "2 56.0 ;\n",
      "2 51.0 good\n",
      "2 49.0 airline\n",
      "2 44.0 today\n",
      "2 44.0 amazing\n",
      "2 43.0 time\n",
      "2 42.0 us\n",
      "2 40.0 got\n",
      "2 40.0 crew\n",
      "2 39.0 &\n",
      "2 38.0 n't\n",
      "2 38.0 fly\n",
      "2 36.0 get\n",
      "2 33.0 help\n",
      "2 32.0 made\n",
      "2 32.0 amp\n",
      "2 32.0 ...\n",
      "2 31.0 flying\n",
      "2 30.0 'm\n",
      "2 29.0 response\n",
      "2 29.0 gate\n",
      "2 28.0 ever\n",
      "2 28.0 ?\n",
      "2 27.0 new\n",
      "2 27.0 day\n",
      "2 27.0 appreciate\n",
      "2 26.0 work\n",
      "2 26.0 see\n",
      "2 26.0 ''\n",
      "2 25.0 'll\n",
      "2 24.0 flights\n",
      "2 24.0 back\n",
      "2 23.0 nice\n",
      "2 23.0 like\n",
      "2 23.0 know\n",
      "2 23.0 home\n",
      "2 22.0 yes\n",
      "2 22.0 would\n",
      "2 22.0 tonight\n",
      "2 22.0 staff\n",
      "2 22.0 plane\n",
      "2 22.0 helpful\n",
      "2 22.0 first\n",
      "2 22.0 (\n",
      "2 21.0 well\n",
      "2 21.0 're\n",
      "2 20.0 really\n",
      "2 20.0 happy\n",
      "2 20.0 follow\n",
      "2 20.0 attendant\n",
      "2 19.0 u\n",
      "2 19.0 job\n",
      "2 19.0 finally\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "\n",
    "#vec\n",
    "airline_vec = CountVectorizer(min_df=2,\n",
    "                             tokenizer=nltk.word_tokenize, \n",
    "                             stop_words=stopwords.words('english'))\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "#split data\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, \n",
    "    airline_tweets_train.target, \n",
    "    test_size = 0.2\n",
    "    ) \n",
    "\n",
    "#train\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***answer:***\n",
    "- We would expect the following words for each class:\n",
    "-- Negative: cancelled, bad, late, lost,.. --Neutral: on time, arrived, departure, platform,.. --Positive: good, recommend, like,... \n",
    "This is because those words are often seen in the context of bad airplane experiences, regular notifications and recomendations, which relates to negative, neutral and positive experiences in airplanes respectively\n",
    "- 'delayed' and 'cancelled' are in the neutral category, which we would expect to be in the negative category, since a flight being cancelled or delayed has a negative conotation.\n",
    "- We would remove the airline names, as the model could learn to correlate the airline companies to the sentiment. We would also remove some of the special characters not used to express intensity (such as #, &, :), as they are ubiquitous in every class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
