{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>cast</th>\n",
       "      <th>summary_wiki</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>info_json</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Family</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>...</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>History</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Birds of Prey: And the Fantabulous Emancipatio...</td>\n",
       "      <td>[{'nm8698119': [' Paloma Rabinov']}, {'nm43992...</td>\n",
       "      <td>Harley Quinn narrates the events of her life l...</td>\n",
       "      <td>Birds of Prey: And the Fantabulous Emancipatio...</td>\n",
       "      <td>{'Title': 'Birds of Prey: And the Fantabulous ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sonic the Hedgehog</td>\n",
       "      <td>[{'nm9961714': [' Melody Nosipho Niemann']}, {...</td>\n",
       "      <td>Sonic is an extraterrestrial blue hedgehog who...</td>\n",
       "      <td>Sonic the Hedgehog</td>\n",
       "      <td>{'Title': 'Sonic the Hedgehog', 'Year': '2020'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Vikings</td>\n",
       "      <td>[{'nm1573253': [' Alexander Ludwig']}, {'nm540...</td>\n",
       "      <td>The series is inspired by the tales of the Nor...</td>\n",
       "      <td>Vikings</td>\n",
       "      <td>{'Title': 'Vikings', 'Year': '2013–', 'Rated':...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Star Trek: Picard</td>\n",
       "      <td>[{'nm0403335': [' Michelle Hurd']}, {'nm108505...</td>\n",
       "      <td>Many years have passed since Data's demise. Fo...</td>\n",
       "      <td>Star Trek: Picard</td>\n",
       "      <td>{'Title': 'Star Trek: Picard', 'Year': '2020–'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Fantasy Island</td>\n",
       "      <td>[{'nm0740264': [' Michael Rooker']}, {'nm10666...</td>\n",
       "      <td>This article's plot summary may be too long or...</td>\n",
       "      <td>Fantasy Island</td>\n",
       "      <td>{'Title': 'Fantasy Island', 'Year': '2020', 'R...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>1663</td>\n",
       "      <td>1838</td>\n",
       "      <td>High Society</td>\n",
       "      <td>[{'nm0526184': [' John Lund']}, {'nm0000038': ...</td>\n",
       "      <td>Successful jazz musician C.K. Dexter Haven (wi...</td>\n",
       "      <td>High Society</td>\n",
       "      <td>{'Title': 'High Society', 'Year': '1956', 'Rat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>1664</td>\n",
       "      <td>1839</td>\n",
       "      <td>Chi-Raq</td>\n",
       "      <td>[{'nm1869008': [' Anya Engel-Adams']}, {'nm000...</td>\n",
       "      <td>In Chicago's Southside, as the events are narr...</td>\n",
       "      <td>Chi-Raq</td>\n",
       "      <td>{'Title': 'Chi-Raq', 'Year': '2015', 'Rated': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>1665</td>\n",
       "      <td>1840</td>\n",
       "      <td>Om Shanti Om</td>\n",
       "      <td>[{'nm0082848': [' Bindu']}, {'nm2061852': [' A...</td>\n",
       "      <td>In 1977 , Om Prakash Makhija, a junior artist ...</td>\n",
       "      <td>Om Shanti Om</td>\n",
       "      <td>{'Title': 'Om Shanti Om', 'Year': '2007', 'Rat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>1667</td>\n",
       "      <td>1842</td>\n",
       "      <td>Teletubbies</td>\n",
       "      <td>[{'nm0799619': [' John Simmit']}, {'nm0266623'...</td>\n",
       "      <td>The programme takes place in a grassy, floral ...</td>\n",
       "      <td>Teletubbies</td>\n",
       "      <td>{'Title': 'Teletubbies', 'Year': '1997–2001', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>1668</td>\n",
       "      <td>1843</td>\n",
       "      <td>Blue Hawaii</td>\n",
       "      <td>[{'nm0443020': [' Christian Kay']}, {'nm001244...</td>\n",
       "      <td>Chadwick Gates (Elvis Presley) has just gotten...</td>\n",
       "      <td>Blue Hawaii</td>\n",
       "      <td>{'Title': 'Blue Hawaii', 'Year': '1961', 'Rate...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1580 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  \\\n",
       "0              0             0   \n",
       "1              1             1   \n",
       "2              2             2   \n",
       "3              3             3   \n",
       "4              4             4   \n",
       "...          ...           ...   \n",
       "1575        1663          1838   \n",
       "1576        1664          1839   \n",
       "1577        1665          1840   \n",
       "1578        1667          1842   \n",
       "1579        1668          1843   \n",
       "\n",
       "                                               movie_id  \\\n",
       "0     Birds of Prey: And the Fantabulous Emancipatio...   \n",
       "1                                    Sonic the Hedgehog   \n",
       "2                                               Vikings   \n",
       "3                                     Star Trek: Picard   \n",
       "4                                        Fantasy Island   \n",
       "...                                                 ...   \n",
       "1575                                       High Society   \n",
       "1576                                            Chi-Raq   \n",
       "1577                                       Om Shanti Om   \n",
       "1578                                        Teletubbies   \n",
       "1579                                        Blue Hawaii   \n",
       "\n",
       "                                                   cast  \\\n",
       "0     [{'nm8698119': [' Paloma Rabinov']}, {'nm43992...   \n",
       "1     [{'nm9961714': [' Melody Nosipho Niemann']}, {...   \n",
       "2     [{'nm1573253': [' Alexander Ludwig']}, {'nm540...   \n",
       "3     [{'nm0403335': [' Michelle Hurd']}, {'nm108505...   \n",
       "4     [{'nm0740264': [' Michael Rooker']}, {'nm10666...   \n",
       "...                                                 ...   \n",
       "1575  [{'nm0526184': [' John Lund']}, {'nm0000038': ...   \n",
       "1576  [{'nm1869008': [' Anya Engel-Adams']}, {'nm000...   \n",
       "1577  [{'nm0082848': [' Bindu']}, {'nm2061852': [' A...   \n",
       "1578  [{'nm0799619': [' John Simmit']}, {'nm0266623'...   \n",
       "1579  [{'nm0443020': [' Christian Kay']}, {'nm001244...   \n",
       "\n",
       "                                           summary_wiki  \\\n",
       "0     Harley Quinn narrates the events of her life l...   \n",
       "1     Sonic is an extraterrestrial blue hedgehog who...   \n",
       "2     The series is inspired by the tales of the Nor...   \n",
       "3     Many years have passed since Data's demise. Fo...   \n",
       "4     This article's plot summary may be too long or...   \n",
       "...                                                 ...   \n",
       "1575  Successful jazz musician C.K. Dexter Haven (wi...   \n",
       "1576  In Chicago's Southside, as the events are narr...   \n",
       "1577  In 1977 , Om Prakash Makhija, a junior artist ...   \n",
       "1578  The programme takes place in a grassy, floral ...   \n",
       "1579  Chadwick Gates (Elvis Presley) has just gotten...   \n",
       "\n",
       "                                             movie_name  \\\n",
       "0     Birds of Prey: And the Fantabulous Emancipatio...   \n",
       "1                                    Sonic the Hedgehog   \n",
       "2                                               Vikings   \n",
       "3                                     Star Trek: Picard   \n",
       "4                                        Fantasy Island   \n",
       "...                                                 ...   \n",
       "1575                                       High Society   \n",
       "1576                                            Chi-Raq   \n",
       "1577                                       Om Shanti Om   \n",
       "1578                                        Teletubbies   \n",
       "1579                                        Blue Hawaii   \n",
       "\n",
       "                                              info_json  Animation  Family  \\\n",
       "0     {'Title': 'Birds of Prey: And the Fantabulous ...          0       0   \n",
       "1     {'Title': 'Sonic the Hedgehog', 'Year': '2020'...          0       1   \n",
       "2     {'Title': 'Vikings', 'Year': '2013–', 'Rated':...          0       0   \n",
       "3     {'Title': 'Star Trek: Picard', 'Year': '2020–'...          0       0   \n",
       "4     {'Title': 'Fantasy Island', 'Year': '2020', 'R...          0       0   \n",
       "...                                                 ...        ...     ...   \n",
       "1575  {'Title': 'High Society', 'Year': '1956', 'Rat...          0       0   \n",
       "1576  {'Title': 'Chi-Raq', 'Year': '2015', 'Rated': ...          0       0   \n",
       "1577  {'Title': 'Om Shanti Om', 'Year': '2007', 'Rat...          0       0   \n",
       "1578  {'Title': 'Teletubbies', 'Year': '1997–2001', ...          0       1   \n",
       "1579  {'Title': 'Blue Hawaii', 'Year': '1961', 'Rate...          0       0   \n",
       "\n",
       "      Fantasy  ...  War  Western  Adventure  Horror  Drama  Romance  Action  \\\n",
       "0           0  ...    0        0          1       0      0        0       1   \n",
       "1           0  ...    0        0          1       0      0        0       1   \n",
       "2           0  ...    1        0          1       0      1        1       1   \n",
       "3           0  ...    0        0          1       0      1        0       1   \n",
       "4           0  ...    0        0          1       1      0        0       0   \n",
       "...       ...  ...  ...      ...        ...     ...    ...      ...     ...   \n",
       "1575        0  ...    0        0          0       0      0        1       0   \n",
       "1576        0  ...    0        0          0       0      1        0       0   \n",
       "1577        0  ...    0        0          0       0      1        1       1   \n",
       "1578        1  ...    0        0          0       0      0        0       0   \n",
       "1579        0  ...    0        0          0       0      0        0       0   \n",
       "\n",
       "      Crime  Comedy  History  \n",
       "0         1       0        0  \n",
       "1         0       0        0  \n",
       "2         0       0        1  \n",
       "3         0       0        0  \n",
       "4         0       1        0  \n",
       "...     ...     ...      ...  \n",
       "1575      0       1        0  \n",
       "1576      1       1        0  \n",
       "1577      0       1        0  \n",
       "1578      0       0        0  \n",
       "1579      0       1        0  \n",
       "\n",
       "[1580 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('genre_filtered_movie.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing nan items\n",
    "nan_items = []\n",
    "for i,d in data.iterrows():\n",
    "    if type(d['summary_wiki'])!= str:\n",
    "        nan_items.append(i)\n",
    "data = data.drop(nan_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing names\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "#filter out names through nerc\n",
    "def remove_names(text):\n",
    "    document = nlp(text)\n",
    "    ents = [e.text for e in document.ents if e.label_ != 'PERSON']\n",
    "    return \" \".join([item.text for item in document if item.text not in ents])\n",
    "\n",
    "data['summary_no_names'] = data['summary_wiki'].apply(lambda x: remove_names(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "genres = ['Animation', 'Family', 'Fantasy', 'Mystery',\n",
    "       'Sci-Fi', 'Thriller', 'Biography', 'Musical', 'War', 'Western',\n",
    "       'Adventure', 'Horror', 'Drama', 'Romance', 'Action', 'Crime', 'Comedy',\n",
    "       'History']\n",
    "\n",
    "sample_movie_indexes = []\n",
    "for g in genres:\n",
    "    for i,m in data[data[g]==1].sample(2).iterrows():\n",
    "        sample_movie_indexes.append(i)\n",
    "\n",
    "train_index = [x for x in range(len(data)) if x not in sample_movie_indexes]\n",
    "\n",
    "test_df = data.iloc[sample_movie_indexes, :]\n",
    "data = data.iloc[train_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "def tokenize(content):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",\" \", content)\n",
    "    lower_case = letters_only.lower()\n",
    "    tokens = word_tokenize(lower_case)\n",
    "    words = [w for w in tokens if not w in stop_words]\n",
    "    stems = [stemmer.lemmatize(word) for word in words]\n",
    "    return(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary and incorporating 1 to n-gram  -> change to tf-idf\n",
    "stemmer = WordNetLemmatizer()\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2),\n",
    "                             lowercase = True,\n",
    "                             tokenizer=tokenize,\n",
    "                             preprocessor = None,\n",
    "                             max_features=5000)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word',tokenizer = tokenize, lowercase = True)#, #ngram_range=(1, 2),\n",
    "#                                    lowercase=True, tokenizer=tokenize,\n",
    "#                                    preprocessor=None, max_features=10000)\n",
    "\n",
    "\n",
    "#bag of words\n",
    "bow = vectorizer.fit_transform(data['summary_no_names'])\n",
    "\n",
    "# vectorizer.vocabulary_\n",
    "tfidf = tfidf_vect.fit_transform(data['summary_no_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  ['police', 'season', 'team', 'murder', 'agent', 'case', 'series', 'death', 'world', 'crime', 'also', 'new', 'would', 'evidence', 'life'] \n",
      "\n",
      "1 :  ['peter', 'john', 'kate', 'jane', 'lucy', 'elizabeth', 'mr', 'claire', 'jean', 'bill', 'lara jean', 'lara', 'owen', 'daughter', 'judy'] \n",
      "\n",
      "2 :  ['george', 'su', 'family', 'mother', 'orry', 'house', 'carl', 'find', 'train', 'jo', 'home', 'cooper', 'nancy', 'tell', 'return'] \n",
      "\n",
      "3 :  ['sarah', 'joe', 'jim', 'edward', 'tom', 'paul', 'danny', 'vampire', 'tommy', 'find', 'jay', 'back', 'go', 'brother', 'kill'] \n",
      "\n",
      "4 :  ['money', 'car', 'max', 'police', 'find', 'take', 'drug', 'get', 'kill', 'go', 'escape', 'tell', 'chris', 'make', 'back'] \n",
      "\n",
      "5 :  ['men', 'kill', 'town', 'soldier', 'war', 'john', 'killed', 'back', 'force', 'take', 'army', 'order', 'shoot', 'gang', 'return'] \n",
      "\n",
      "6 :  ['new', 'friend', 'life', 'tell', 'family', 'love', 'go', 'father', 'school', 'home', 'take', 'later', 'day', 'get', 'show'] \n",
      "\n",
      "7 :  ['return', 'find', 'king', 'father', 'escape', 'take', 'power', 'help', 'back', 'reveals', 'kill', 'year', 'world', 'human', 'group'] \n",
      "\n",
      "8 :  ['ship', 'frank', 'crew', 'charlie', 'alien', 'kill', 'escape', 'team', 'attack', 'attempt', 'island', 'base', 'creature', 'kirk', 'dr'] \n",
      "\n",
      "9 :  ['find', 'house', 'home', 'kill', 'man', 'body', 'child', 'death', 'woman', 'see', 'room', 'later', 'go', 'tell', 'take'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_topics = 10\n",
    "lda = LatentDirichletAllocation(n_components=nb_topics, max_iter=20,\n",
    "                                learning_method='batch')\n",
    "document_topics = lda.fit_transform(bow)\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "def print_topics(topics, feature_names, sorting, topics_per_chunk=6,\n",
    "                 n_words=20):\n",
    "    for i in range(0, len(topics), topics_per_chunk):\n",
    "        these_topics = topics[i: i + topics_per_chunk]\n",
    "        len_this_chunk = len(these_topics)\n",
    "        words = []\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                words.append(feature_names[sorting[these_topics, i]])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    #setting up word dictionary for comparison\n",
    "    word_dict = {}\n",
    "    for i in topics:\n",
    "        word_dict.update({i : [word[i] for word in words]})\n",
    "    \n",
    "    return word_dict\n",
    "\n",
    "lda_topics = print_topics(topics=range(nb_topics), feature_names=feature_names, sorting=sorting, topics_per_chunk=nb_topics, n_words=15)\n",
    "\n",
    "for i in range(nb_topics):\n",
    "    print(i,': ' ,lda_topics[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27042.78108509934"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clusters'] = np.argmax(document_topics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x16d3f4e09c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['clusters'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_word_cloud(lda_model,feature_names, topic):\n",
    "    \n",
    "    word_matrix = lda_model.components_[topic]\n",
    "    word_dict = dict(zip(feature_names, word_matrix))\n",
    "    wc = WordCloud(width=800, height=400, max_words=200).generate_from_frequencies(word_dict)\n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    #plt.imshow(wc, interpolation='bilinear')\n",
    "    #plt.axis('off')\n",
    "    #plt.show()\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_actors_clustering(lda_model, document_topics, word_vectorizer, text):\n",
    "    feature_vector = word_vectorizer.transform([text])\n",
    "    topic_distribution = lda_model.transform(feature_vector)\n",
    "    topic = topic_distribution[0].argmax()\n",
    "    related_documents = []\n",
    "    clusters = np.argmax(document_topics, axis = 1)\n",
    "    for i, e in enumerate(clusters):\n",
    "        if e == topic:\n",
    "            related_documents.append(i)\n",
    "    return (topic, related_documents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a,b):\n",
    "    \n",
    "    return np.sqrt(np.power(a-b,2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(document_topics, word_vectorizer, text, k = 10):\n",
    "    feature_vector = word_vectorizer.transform([text]).toarray()[0]\n",
    "    distances = []\n",
    "    for document in document_topics.toarray():\n",
    "        distances.append(euclidean(feature_vector, document))\n",
    "    \n",
    "    return np.argsort(np.array(distances))[0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_k_nearest_documents(lda_model, document_topics, word_vectorizer, text, k = 10):\n",
    "    feature_vector = word_vectorizer.transform([text])\n",
    "    topic_distribution = lda_model.transform(feature_vector)[0]\n",
    "    distances = []\n",
    "    for document in document_topics:\n",
    "        distances.append(euclidean(topic_distribution, document))\n",
    "    \n",
    "    return np.argsort(np.array(distances))[0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast = list(data['cast'])\n",
    "def retrieve_cast(cast, document_indexes):\n",
    "    actors = dict()\n",
    "    \n",
    "    for index in document_indexes:\n",
    "        for d in cast[index]:\n",
    "            for v in d.values():\n",
    "                actor = v[0][1:]\n",
    "                if actor not in actors:\n",
    "                    actors[actor] = 1\n",
    "                else:\n",
    "                    actors[actor]+=1\n",
    "    return sorted(actors.items(), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_actors(actors):\n",
    "    return [actor[0]+' : '+ str(actor[1])+'\\n' for actor in actors][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "lda training\n",
      "word cloud\n",
      "experimenting\n",
      "lda training\n",
      "word cloud\n",
      "experimenting\n",
      "lda training\n"
     ]
    }
   ],
   "source": [
    "# Selecting the vectorizer\n",
    "bow = CountVectorizer(lowercase = True,tokenizer=tokenize)\n",
    "tfidf = TfidfVectorizer(analyzer = 'word',tokenizer = tokenize, lowercase = True)\n",
    "\n",
    "vectorizers = [tfidf, bow]\n",
    "                \n",
    "# Selecting number of topics\n",
    "number_of_topics = list(range(2,20))\n",
    "                        \n",
    "# Selecting the k nearest\n",
    "k_nearest = [3,5,10]\n",
    "\n",
    "\n",
    "for v in vectorizers:\n",
    "    path_string ='./results/'\n",
    "    if 'CountVectorizer' in str(vectorizer):\n",
    "        path_string+='bow/'\n",
    "    else:\n",
    "        path_string+='tfidf/'\n",
    "        \n",
    "    feature_matrix = v.fit_transform(data['summary_no_names'])\n",
    "    # baseline\n",
    "    baseline_path = path_string+'baseline.txt'\n",
    "    print('baseline',baseline_path )\n",
    "    baseline_file = open(baseline_path, 'w+', encoding = 'utf-8')\n",
    "    baseline_file.write('BASELINE FILE \\n')\n",
    "    for i,movie in test_df.iterrows():\n",
    "        baseline_file.write('***************\\n')\n",
    "        baseline_file.write('MOVIE :'+movie['movie_id']+'\\n')\n",
    "        b = log_actors(retrieve_cast(cast,baseline(feature_matrix, v, movie['summary_wiki'])))\n",
    "        baseline_file.writelines(b)\n",
    "    baseline_file.close()\n",
    "    \n",
    "    for t in number_of_topics:\n",
    "        experiment_path=path_string+\"topic\"+str(t)+\"/\"\n",
    "        if not os.path.exists(experiment_path):\n",
    "            os.mkdir(experiment_path)\n",
    "            \n",
    "        lda = LatentDirichletAllocation(n_components = t)\n",
    "        print('lda training')\n",
    "        document_topics = lda.fit_transform(feature_matrix)\n",
    "        feature_names = np.array(v.get_feature_names())\n",
    "        \n",
    "        clusters =  np.argmax(document_topics, axis=1)\n",
    "        cluster_counts = [0 for _ in range(t)]\n",
    "        for c in clusters:\n",
    "            cluster_counts[c]+=1\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.bar(range(t), cluster_counts)\n",
    "        plt.savefig(experiment_path+'cluster_distribution.png')\n",
    "        \n",
    "        word_cloud_path = experiment_path+'wordclouds/'\n",
    "        print('word cloud')\n",
    "        if not os.path.exists(word_cloud_path):\n",
    "            os.mkdir(word_cloud_path)\n",
    "            \n",
    "        for w in range(t):\n",
    "            word_cloud = topic_word_cloud(lda, feature_names, w)\n",
    "            word_cloud.to_file(word_cloud_path+str(w)+'.png')\n",
    "        \n",
    "        print('experimenting')\n",
    "        for k in k_nearest:\n",
    "            experiment_file_name =experiment_path+'k_'+str(k)+'.txt'\n",
    "            experiment_file = open(experiment_file_name, 'w+', encoding = 'utf-8')\n",
    "            experiment_file.write('Experiment \\n')\n",
    "            experiment_file.write(\"## METADATA ## \\n\")\n",
    "            experiment_file.write(\"Vectorizer :\"+ str(v)+'\\n')\n",
    "            experiment_file.write(\"N_topics :\"+ str(t)+'\\n')\n",
    "            experiment_file.write(\"K_nearest :\"+ str(k)+'\\n')\n",
    "            experiment_file.write(\"##############\")\n",
    "            \n",
    "            for i,movie in test_df.iterrows():\n",
    "                experiment_file.write('***********************\\n')\n",
    "                experiment_file.write('MOVIE :'+ movie['movie_id']+'\\n')\n",
    "                b = log_actors(retrieve_cast(cast,get_list_of_k_nearest_documents(lda,document_topics, v, movie['summary_wiki'], k = k)))\n",
    "                experiment_file.writelines(b)\n",
    "            \n",
    "            experiment_file.close()\n",
    "        \n",
    "        experiment_file_name = experiment_path+'cluster.txt'\n",
    "        experiment_file = open(experiment_file_name, 'w+', encoding = 'utf-8')\n",
    "        experiment_file.write('Experiment cluster \\n')\n",
    "        \n",
    "        for i,movie in test_df.iterrows():\n",
    "            experiment_file.write('***********************\\n')\n",
    "            experiment_file.write('MOVIE :'+ movie['movie_id']+'\\n')\n",
    "            cl, doc_index = get_list_of_actors_clustering(lda, document_topics, v, movie['summary_wiki'])\n",
    "            experiment_file.write('CLUSTER :' +str(cl)+'\\n')\n",
    "            b = log_actors(retrieve_cast(cast,doc_index))\n",
    "            experiment_file.writelines(b)\n",
    "        \n",
    "        experiment_file.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None,\\n                strip_accents=None, token_pattern='(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b',\\n                tokenizer=<function tokenize at 0x0000016D34142F78>,\\n                vocabulary=None)\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(bow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
